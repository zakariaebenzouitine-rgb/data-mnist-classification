{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¯ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸš€ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤” <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "ğŸ”¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the ğŸ“š [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“š Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "ğŸ’¾ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 12:23:58.789471: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-20 12:23:58.792847: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-20 12:23:58.804744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-20 12:23:58.827812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-20 12:23:58.827854: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-20 12:23:58.843137: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 12:23:59.960215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question: Let's have look at some handwritten digits of this MNIST dataset.** â“\n",
    "\n",
    "ğŸ–¨ Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "ğŸ’¡*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ğŸ¤¨ Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x71cee9ff0800>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â—ï¸ **Neural Networks converge faster when the input data is somehow normalized** â—ï¸\n",
    "\n",
    "ğŸ‘©ğŸ»â€ğŸ« How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 ğŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question â“ As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘† Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> â—ï¸  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> ğŸ§‘ğŸ»â€ğŸ« The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "ğŸ•µğŸ»This last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question: expanding dimensions** â“\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.ops import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 12:24:02.277057: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 376320000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "X_train = expand_dims(X_train, axis=-1)\n",
    "X_test = expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "ğŸ‘‰ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "â“ **Question: encoding the labels** â“ \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([28, 28, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "â“ **Question: CNN Architecture and compilation** â“\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, Input, layers, models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(28,28,1)))\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, kernel_size=(4,4), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, kernel_size=(4,4), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer= 'adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question: number of trainable parameters in a convolutional layer** â“ \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,010</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m8\u001b[0m)      â”‚           \u001b[38;5;34m136\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m8\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m)     â”‚         \u001b[38;5;34m2,064\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚         \u001b[38;5;34m4,010\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚           \u001b[38;5;34m110\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,320</span> (24.69 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,320\u001b[0m (24.69 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,320</span> (24.69 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,320\u001b[0m (24.69 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question: training a CNN** â“ \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.6779 - loss: 0.9467 - val_accuracy: 0.9541 - val_loss: 0.1529\n",
      "Epoch 2/5\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.9565 - loss: 0.1435 - val_accuracy: 0.9652 - val_loss: 0.1204\n",
      "Epoch 3/5\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9702 - loss: 0.0964 - val_accuracy: 0.9702 - val_loss: 0.0955\n",
      "Epoch 4/5\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.9765 - loss: 0.0767 - val_accuracy: 0.9741 - val_loss: 0.0857\n",
      "Epoch 5/5\n",
      "\u001b[1m1313/1313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.9811 - loss: 0.0596 - val_accuracy: 0.9759 - val_loss: 0.0781\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_cat, epochs=5, validation_split=0.3, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question: How many iterations does the CNN perform per epoch** â“\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses ğŸ˜‰_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "ğŸ‘‰ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question: Evaluating your CNN** â“ \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.058616168797016144, 0.9817000031471252]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‰ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "ğŸ”¥ You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ **Congratulations!**\n",
    "\n",
    "ğŸ’¾ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "ğŸš€ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
